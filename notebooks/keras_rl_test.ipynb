{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 (default, Oct  3 2017, 21:45:48) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import numpy\n",
    "import rl\n",
    "import scipy.sparse\n",
    "import skimage.io\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 8\n",
    "inter_neurons = 128\n",
    "output_neurons = 8\n",
    "max_history = 16\n",
    "hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_layer = Input((inter_neurons + output_neurons, max_history, input_neurons + inter_neurons + 2))\n",
    "# x = TimeDistributed(Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat'))(input_layer)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = TimeDistributed(Bidirectional(LSTM(hidden_size), merge_mode='concat'))(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# output_layer = TimeDistributed(Dense(input_neurons + inter_neurons, activation='tanh'))(x)\n",
    "# model = Model(input_layer, output_layer)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((None, 4))\n",
    "x = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(input_layer)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(hidden_size), merge_mode='concat')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(3, activation='softmax')(x)\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SNN(gym.Env):\n",
    "    \n",
    "#     def __init__(self, specification):\n",
    "#         self.specification = specification\n",
    "#         self.action_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "#         self.observation_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "#         self.potential_matrix = numpy.zeros((self.specification['input_neurons'] + self.specification['inter_neurons'] + self.specification['output_neurons'],))\n",
    "#         self.weight_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "#         self.weight_mask = numpy.ones_like(self.weight_matrix, dtype=numpy.uint8)\n",
    "#         self.weight_mask[-self.specification['output_neurons']:, :self.specification['input_neurons']] = 0\n",
    "#         numpy.fill_diagonal(self.weight_mask[:self.specification['inter_neurons'], -self.specification['inter_neurons']:], 0)\n",
    "#         self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "#         self.random_seed = None\n",
    "#         self.next_input = None\n",
    "#         self.previous_reward = None\n",
    "    \n",
    "#     def interconnect(self, weight_density):\n",
    "#         sparse_matrix = scipy.sparse.random(self.weight_matrix.shape[0], self.weight_matrix.shape[1], density=weight_density, random_state=self.random_seed)\n",
    "#         sparse_matrix.data *= 2\n",
    "#         sparse_matrix.data -= 1\n",
    "#         self.weight_matrix = numpy.multiply(sparse_matrix.toarray(), self.weight_mask)\n",
    "    \n",
    "#     def close(self):\n",
    "#         self.specification['environment'].close()\n",
    "        \n",
    "#     def reset(self):\n",
    "#         self.next_input = self.specification['environment'].reset()\n",
    "#         self.previous_reward = None\n",
    "#         self.potential_matrix[:] = 0\n",
    "#         if self.specification['neuroplasticity']:\n",
    "#             self.history_matrix[:, :, :] = 0\n",
    "#         return self.history_matrix\n",
    "    \n",
    "#     def seed(self, seed):\n",
    "#         self.random_seed = seed\n",
    "#         return self.specification['environment'].seed(self.random_seed)\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         if self.specification['neuroplasticity']:\n",
    "#             self.weight_matrix = numpy.clip(numpy.multiply(numpy.add(self.weight_matrix, self.specification['learning_rate'] * action), self.weight_mask), -1, 1)\n",
    "#         self.potential_matrix[:self.specification['input_neurons']] = numpy.add(self.potential_matrix[:self.specification['input_neurons']], self.next_input)\n",
    "#         firing_matrix = numpy.vectorize(lambda x: x >= 1)(self.potential_matrix)\n",
    "#         for i in range(self.specification['inter_neurons'] + self.specification['output_neurons']):\n",
    "#             pos = self.specification['input_neurons'] + i\n",
    "#             deltas = numpy.multiply(firing_matrix[:-self.specification['output_neurons']], self.weight_matrix[i])\n",
    "#             if self.specification['neuroplasticity']:\n",
    "#                 self.history_matrix[i, self.specification['max_history'] - 1, :] = numpy.concatenate([self.potential_matrix[pos:pos + 1], firing_matrix[pos:pos + 1], deltas])\n",
    "#             self.potential_matrix[pos] += numpy.sum(deltas)\n",
    "#         self.potential_matrix = numpy.clip(numpy.multiply(self.potential_matrix, numpy.invert(firing_matrix)), -1, 1)\n",
    "#         if self.specification['neuroplasticity']:\n",
    "#             self.history_matrix = numpy.roll(self.history_matrix, 1, axis=1)\n",
    "#         self.next_input, current_reward, terminal, _ = self.specification['environment'].step(firing_matrix[-self.specification['output_neurons']:])\n",
    "#         reward = 0 if self.previous_reward is None else current_reward - self.previous_reward\n",
    "#         self.previous_reward = current_reward\n",
    "#         return self.history_matrix, reward, terminal, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(gym.Env):\n",
    "    \n",
    "    def __init__(self, specification):\n",
    "        self.specification = specification\n",
    "#         self.action_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "#         self.observation_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (self.specification['max_history'], 4))\n",
    "        self.potential_matrix = numpy.zeros((self.specification['input_neurons'] + self.specification['inter_neurons'] + self.specification['output_neurons'],))\n",
    "        self.weight_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "        self.weight_mask = numpy.ones_like(self.weight_matrix, dtype=numpy.uint8)\n",
    "        self.weight_mask[-self.specification['output_neurons']:, :self.specification['input_neurons']] = 0\n",
    "        numpy.fill_diagonal(self.weight_mask[:self.specification['inter_neurons'], -self.specification['inter_neurons']:], 0)\n",
    "#         self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "        self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons'], self.specification['max_history'], 4))\n",
    "        self.neuron_idx = 0\n",
    "        self.weight_idx = 0\n",
    "        self.random_seed = None\n",
    "        self.next_input = None\n",
    "        self.previous_reward = None\n",
    "    \n",
    "    def interconnect(self, weight_density):\n",
    "        sparse_matrix = scipy.sparse.random(self.weight_matrix.shape[0], self.weight_matrix.shape[1], density=weight_density, random_state=self.random_seed)\n",
    "        sparse_matrix.data *= 2\n",
    "        sparse_matrix.data -= 1\n",
    "        self.weight_matrix = numpy.multiply(sparse_matrix.toarray(), self.weight_mask)\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        self.specification['environment'].close()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.next_input = self.specification['environment'].reset()\n",
    "#         self.neuron_idx = 0\n",
    "#         self.weight_idx = 0\n",
    "        self.previous_reward = None\n",
    "#         self.potential_matrix[:] = 0\n",
    "#         if self.specification['neuroplasticity']:\n",
    "#             self.history_matrix[:, :, :] = 0\n",
    "        return self.history_matrix[self.neuron_idx, self.weight_idx, :, :]\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.specification['environment'].render(mode)\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self.random_seed = seed\n",
    "        return self.specification['environment'].seed(self.random_seed)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.neuron_idx += 1\n",
    "        self.weight_idx += 1\n",
    "        if self.neuron_idx == self.weight_matrix.shape[0]:\n",
    "            self.neuron_idx = 0\n",
    "        if self.weight_idx == self.weight_matrix.shape[1]:\n",
    "            self.weight_idx = 0\n",
    "#         print(action)\n",
    "#         print(self.neuron_idx, self.weight_idx)\n",
    "        if self.specification['neuroplasticity']:\n",
    "#             self.weight_matrix = numpy.clip(numpy.multiply(numpy.add(self.weight_matrix, self.specification['learning_rate'] * action), self.weight_mask), -1, 1)\n",
    "            self.weight_matrix[self.neuron_idx, self.weight_idx] += self.specification['learning_rate'] * (action - 1)\n",
    "            self.weight_matrix = numpy.clip(numpy.multiply(self.weight_matrix, self.weight_mask), -1, 1)\n",
    "        state = numpy.zeros_like(self.history_matrix[self.neuron_idx, self.weight_idx, :, :])\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        if self.neuron_idx == 0 and self.weight_idx == 0:\n",
    "            self.potential_matrix[:self.specification['input_neurons']] = numpy.add(self.potential_matrix[:self.specification['input_neurons']], self.next_input)\n",
    "            firing_matrix = numpy.vectorize(lambda x: x >= 1)(self.potential_matrix)\n",
    "            for i in range(self.specification['inter_neurons'] + self.specification['output_neurons']):\n",
    "                pos = self.specification['input_neurons'] + i\n",
    "                deltas = numpy.multiply(firing_matrix[:-self.specification['output_neurons']], self.weight_matrix[i])\n",
    "                delta = numpy.sum(deltas)\n",
    "                if self.specification['neuroplasticity']:\n",
    "                    self.history_matrix[i, self.weight_idx, self.specification['max_history'] - 1, :] = numpy.array([deltas[self.weight_idx], delta, self.potential_matrix[pos], firing_matrix[pos]])\n",
    "                self.potential_matrix[pos] += delta\n",
    "            self.potential_matrix = numpy.clip(numpy.multiply(self.potential_matrix, numpy.invert(firing_matrix)), -1, 1)\n",
    "            if self.specification['neuroplasticity']:\n",
    "                self.history_matrix = numpy.roll(self.history_matrix, 2, axis=1)\n",
    "            state = self.history_matrix[self.neuron_idx, self.weight_idx, :, :]\n",
    "            self.next_input, reward, terminal, _ = self.specification['environment'].step(firing_matrix[-self.specification['output_neurons']:].astype(int))\n",
    "#             reward = 0 if self.previous_reward is None else current_reward - self.previous_reward\n",
    "#             self.previous_reward = current_reward\n",
    "#         if self.weight_idx == self.specification['input_neurons'] + self.specification['inter_neurons'] - 1:\n",
    "#             self.weight_idx = 0\n",
    "#             if self.neuron_idx == self.specification['inter_neurons'] + self.specification['output_neurons'] - 1:\n",
    "#                 self.neuron_idx = 0\n",
    "#             else:\n",
    "# #                 print(self.neuron_idx)\n",
    "#                 self.neuron_idx += 1\n",
    "#         else:\n",
    "#             self.weight_idx += 1\n",
    "        return state, reward, terminal, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "#         self.action_space = gym.spaces.Box(-1, 1, (2,))\n",
    "        self.action_space = gym.spaces.Box(0, 1, (8,))\n",
    "        self.observation_space = gym.spaces.Box(0, float('inf'), (8,))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = numpy.zeros((8,))\n",
    "        self.idx = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "#         print(action)\n",
    "#         print(actions)\n",
    "        #print(actions)\n",
    "        #actions = numpy.round(actions)\n",
    "#         self.state = numpy.add(self.state, numpy.array([actions[0], actions[1]]))\n",
    "        self.state = numpy.add(self.state, action)\n",
    "        #terminal = True\n",
    "        #ones = numpy.count_nonzero(self.state)\n",
    "        reward = ((self.state[1] - self.state[0]) * (self.state[2] - self.state[3])) * ((self.state[4] - self.state[5]) * (self.state[6] - self.state[7]))\n",
    "        terminal = reward < 0\n",
    "#         self.state = numpy.abs(numpy.add(self.state, actions))\n",
    "#         terminal = min(self.state) <= 0\n",
    "#         reward = self.state[0] ** self.state[1]\n",
    "        self.idx += 1\n",
    "        return numpy.ones(self.state.shape), reward, terminal, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test2(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (1,))\n",
    "        self.state = None\n",
    "        self.idx = None\n",
    "        self.random_seed = None\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self.random_seed = seed\n",
    "        random.seed(seed)\n",
    "        return seed\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = numpy.ones((1,))\n",
    "        self.idx = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "#         print(self.idx)\n",
    "#         self.state[action] = int(not bool(self.state[action]))\n",
    "#         self.state = numpy.zeros((16,))\n",
    "#         self.state[self.idx] = 1\n",
    "        self.idx += 1\n",
    "        terminal = self.idx == 100\n",
    "        if action[0] == 1:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        \n",
    "        return self.state, reward, terminal, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5 # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max])\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "#         assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag * (2 * numpy.argmax(action) - 1) if numpy.count_nonzero(action) == 1 else 0\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        x  = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        output = np.array([x < 0, x > 0, x_dot < 0, x_dot > 0, theta < 0, theta > 0, theta_dot < 0, theta_dot > 0]).astype(int)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return output, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return numpy.zeros((8,))\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer: self.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'SNN'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "#env = gym.make(ENV_NAME)\n",
    "env = SNN({'environment': Test2(), 'input_neurons': 1, 'inter_neurons': 10, 'output_neurons': 1, 'max_history': 16, 'neuroplasticity': True, 'learning_rate': 0.1})\n",
    "numpy.random.seed(0)\n",
    "env.seed(0)\n",
    "env.interconnect(0.1)\n",
    "nb_actions = env.action_space.n\n",
    "#functools.reduce(operator.mul, env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/.local/lib/python3.6/site-packages/skimage/io/_plugins/matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75e6f4fcf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEYCAYAAAA3cc++AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqJQAmZERKEEzIiIQv8PS4ju7RyT2IMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75f0d68eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skimage.io.imshow(env.weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 16, 4)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 16, 64)            9472      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 34,499\n",
      "Trainable params: 34,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "done, took 39.382 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75e9723c18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Reshape((16, 4), input_shape=(1, 16, 4)))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True), merge_mode='concat'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Bidirectional(LSTM(32), merge_mode='concat'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='linear'))\n",
    "print(model.summary())\n",
    "# model = Sequential()\n",
    "# model.add(Reshape((16,), input_shape=(1, 16)))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(nb_actions, activation='linear'))\n",
    "# print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, visualize=False, verbose=2, nb_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SNN({'environment': Test2(), 'input_neurons': 1, 'inter_neurons': 10, 'output_neurons': 1, 'max_history': 16, 'neuroplasticity': True, 'learning_rate': 0.1})\n",
    "numpy.random.seed(0)\n",
    "env.seed(0)\n",
    "env.interconnect(0.1)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    stuff = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(env.potential_matrix.reshape((3, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import gym\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Flatten\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# from rl.agents import *\n",
    "# from rl.memory import *\n",
    "\n",
    "# #ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# # Get the environment and extract the number of actions.\n",
    "# env = Test()\n",
    "# np.random.seed(123)\n",
    "# env.seed(123)\n",
    "\n",
    "# nb_actions = functools.reduce(operator.mul, env.action_space.shape)\n",
    "# obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# # Option 1 : Simple model\n",
    "# # model = Sequential()\n",
    "# # model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# # model.add(Dense(nb_actions))\n",
    "# # model.add(Activation('softmax'))\n",
    "\n",
    "# # Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('tanh'))\n",
    "\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "\n",
    "# # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# # even the metrics!\n",
    "# memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "# agent = DDPGAgent()\n",
    "# cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "#                batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "\n",
    "# cem.compile()\n",
    "\n",
    "# # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# # Ctrl + C.\n",
    "# cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# # After training is done, we save the best weights.\n",
    "# cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# # Finally, evaluate our algorithm for 5 episodes.\n",
    "# cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "#functools.reduce(operator.mul, env.action_space.shape)\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, visualize=False, verbose=1, nb_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "gym.undo_logger_setup()\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "assert len(env.action_space.shape) == 1\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=50000, visualize=True, verbose=1, nb_max_episode_steps=200)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = SNN({'environment': Test(), 'input_neurons': 2, 'inter_neurons': 128, 'output_neurons': 2, 'max_history': 16, 'neuroplasticity': True, 'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.interconnect(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(snn.weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    stuff = snn.step(numpy.zeros((130, 130)))\n",
    "    if stuff[1] > 0:\n",
    "        print(stuff[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(snn.potential_matrix.reshape(11, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx = 0\n",
    "weight_idx = 0\n",
    "for i in range(10000):\n",
    "    neuron_idx += 1\n",
    "    weight_idx += 1\n",
    "    if neuron_idx == 11:\n",
    "        neuron_idx = 0\n",
    "    if weight_idx == 11:\n",
    "        weight_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
