{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# from gym import spaces\n",
    "# from gym.utils import seeding\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class HotterColder(gym.Env):\n",
    "#     \"\"\"Hotter Colder\n",
    "#     The goal of hotter colder is to guess closer to a randomly selected number\n",
    "#     After each step the agent receives an observation of:\n",
    "#     0 - No guess yet submitted (only after reset)\n",
    "#     1 - Guess is lower than the target\n",
    "#     2 - Guess is equal to the target\n",
    "#     3 - Guess is higher than the target\n",
    "#     The rewards is calculated as:\n",
    "#     (min(action, self.number) + self.range) / (max(action, self.number) + self.range)\n",
    "#     Ideally an agent will be able to recognise the 'scent' of a higher reward and\n",
    "#     increase the rate in which is guesses in that direction until the reward reaches\n",
    "#     its maximum\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.range = 1000  # +/- value the randomly select number can be between\n",
    "#         self.bounds = 2000  # Action space bounds\n",
    "\n",
    "#         self.action_space = spaces.Box(low=np.array([-self.bounds]), high=np.array([self.bounds]))\n",
    "#         self.observation_space = spaces.Discrete(4)\n",
    "\n",
    "#         self.number = 0\n",
    "#         self.guess_count = 0\n",
    "#         self.guess_max = 200\n",
    "#         self.observation = 0\n",
    "\n",
    "#         self.seed()\n",
    "#         self.reset()\n",
    "\n",
    "#     def seed(self, seed=None):\n",
    "#         self.np_random, seed = seeding.np_random(seed)\n",
    "#         return [seed]\n",
    "\n",
    "#     def step(self, action):\n",
    "#         assert self.action_space.contains(action)\n",
    "\n",
    "#         if action < self.number:\n",
    "#             self.observation = 1\n",
    "\n",
    "#         elif action == self.number:\n",
    "#             self.observation = 2\n",
    "\n",
    "#         elif action > self.number:\n",
    "#             self.observation = 3\n",
    "\n",
    "#         reward = ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) ** 2\n",
    "\n",
    "#         self.guess_count += 1\n",
    "#         done = self.guess_count >= self.guess_max\n",
    "\n",
    "#         return self.observation, reward[0], done, {\"number\": self.number, \"guesses\": self.guess_count}\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.number = self.np_random.uniform(-self.range, self.range)\n",
    "#         self.guess_count = 0\n",
    "#         self.observation = 0\n",
    "#         return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  # examples/quickstart.py\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from tensorforce.agents import PPOAgent\n",
    "# from tensorforce.execution import Runner\n",
    "# from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "# # Create an OpenAIgym environment\n",
    "# env = OpenAIGym('CartPole-v1', visualize=False)\n",
    "\n",
    "# # Network as list of layers\n",
    "# network_spec = [\n",
    "#     dict(type='dense', size=32, activation='tanh'),\n",
    "#     dict(type='dense', size=32, activation='tanh')\n",
    "# ]\n",
    "\n",
    "# agent = PPOAgent(\n",
    "#     states_spec=env.states,\n",
    "#     actions_spec=env.actions,\n",
    "#     network_spec=network_spec,\n",
    "#     batch_size=4096,\n",
    "#     # BatchAgent\n",
    "#     keep_last_timestep=True,\n",
    "#     # PPOAgent\n",
    "#     step_optimizer=dict(\n",
    "#         type='adam',\n",
    "#         learning_rate=1e-3\n",
    "#     ),\n",
    "#     optimization_steps=10,\n",
    "#     # Model\n",
    "#     scope='ppo',\n",
    "#     discount=0.99,\n",
    "#     # DistributionModel\n",
    "#     distributions_spec=None,\n",
    "#     entropy_regularization=0.01,\n",
    "#     # PGModel\n",
    "#     baseline_mode=None,\n",
    "#     baseline=None,\n",
    "#     baseline_optimizer=None,\n",
    "#     gae_lambda=None,\n",
    "#     # PGLRModel\n",
    "#     likelihood_ratio_clipping=0.2,\n",
    "#     summary_spec=None,\n",
    "#     distributed_spec=None\n",
    "# )\n",
    "\n",
    "# # Create the runner\n",
    "# runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# # Callback function printing episode statistics\n",
    "# def episode_finished(r):\n",
    "#     print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "#                                                                                  reward=r.episode_rewards[-1]))\n",
    "#     return True\n",
    "\n",
    "\n",
    "# # Start learning\n",
    "# runner.run(episodes=3000, max_episode_timesteps=500, episode_finished=episode_finished)\n",
    "# runner.close()\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "#     ep=runner.episode,\n",
    "#     ar=np.mean(runner.episode_rewards[-100:]))\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "#     ep=runner.episode,\n",
    "#     ar=np.mean(runner.episode_rewards[-100:]))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import random\n",
    "\n",
    "# import numpy\n",
    "\n",
    "# from tensorforce import util, TensorForceError\n",
    "# from tensorforce.environments import Environment\n",
    "\n",
    "\n",
    "# class MinimalTest(Environment):\n",
    "\n",
    "#     def __init__(self, specification):\n",
    "#         \"\"\"\n",
    "#         Initializes a minimal test environment, which is used for the unit tests.\n",
    "#         Given a specification of actions types and shapes, the environment states consist\n",
    "#         of the same number of pairs (x, y). The (mean of) an action a gives the next state via (1-a, a),\n",
    "#         and the 'correct' state is always (0, 1).\n",
    "#         Args:\n",
    "#             specification: Takes a list of (type, shape) pairs specifying the action structure of the environment.\n",
    "#         \"\"\"\n",
    "#         self.specification = dict()\n",
    "#         for action_type, shape in specification.items():\n",
    "#             if action_type in ('bool', 'int', 'float', 'bounded'):\n",
    "#                 if isinstance(shape, int):\n",
    "#                     self.specification[action_type] = (shape,)\n",
    "#                 else:\n",
    "#                     self.specification[action_type] = tuple(shape)\n",
    "#             else:\n",
    "#                 raise TensorForceError('Invalid MinimalTest specification.')\n",
    "#         self.single_state_action = (len(specification) == 1)\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return 'MinimalTest'\n",
    "\n",
    "#     def close(self):\n",
    "#         pass\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.state = {action_type: (1.0, 0.0) for action_type in self.specification}\n",
    "#         if self.single_state_action:\n",
    "#             return next(iter(self.state.values()))\n",
    "#         else:\n",
    "#             return dict(self.state)\n",
    "\n",
    "#     def execute(self, actions):\n",
    "#         if self.single_state_action:\n",
    "#             actions = {next(iter(self.specification)): actions}\n",
    "\n",
    "#         reward = 0.0\n",
    "#         for action_type, shape in self.specification.items():\n",
    "#             if action_type == 'bool' or action_type == 'int':\n",
    "#                 correct = numpy.sum(actions[action_type])\n",
    "#                 overall = util.prod(shape)\n",
    "#                 self.state[action_type] = ((overall - correct) / overall, correct / overall)\n",
    "#             elif action_type == 'float' or action_type == 'bounded':\n",
    "#                 step = numpy.sum(actions[action_type]) / util.prod(shape)\n",
    "#                 self.state[action_type] = max(self.state[action_type][0] - step, 0.0), min(self.state[action_type][1] + step, 1.0)\n",
    "#             reward += max(min(self.state[action_type][1], 1.0), 0.0)\n",
    "\n",
    "#         terminal = random() < 0.25\n",
    "#         if self.single_state_action:\n",
    "#             return next(iter(self.state.values())), terminal, reward\n",
    "#         else:\n",
    "#             reward = reward / len(self.specification)\n",
    "#             return dict(self.state), terminal, reward\n",
    "\n",
    "#     @property\n",
    "#     def states(self):\n",
    "#         if self.single_state_action:\n",
    "#             return dict(shape=2, type='float')\n",
    "#         else:\n",
    "#             return {action_type: dict(shape=(2,), type='float') for action_type in self.specification}\n",
    "\n",
    "#     @property\n",
    "#     def actions(self):\n",
    "#         if self.single_state_action:\n",
    "#             action_type = next(iter(self.specification))\n",
    "#             if action_type == 'int':\n",
    "#                 return dict(type='int', num_actions=2)\n",
    "#             elif action_type == 'bounded':\n",
    "#                 return dict(type='float', min_value=-0.5, max_value=1.5)\n",
    "#             else:\n",
    "#                 return dict(type=action_type)\n",
    "#         else:\n",
    "#             actions = dict()\n",
    "#             for action_type, shape in self.specification.items():\n",
    "#                 if action_type == 'int':\n",
    "#                     actions[action_type] = dict(type='int', shape=shape, num_actions=2)\n",
    "#                 elif action_type == 'bounded':\n",
    "#                     actions[action_type] = dict(type='float', shape=shape, min_value=-0.5, max_value=1.5)\n",
    "#                 else:\n",
    "#                     actions[action_type] = dict(type=action_type, shape=shape)\n",
    "#             return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import scipy.sparse\n",
    "import skimage.io\n",
    "from tensorforce import util, TensorForceError\n",
    "from tensorforce.environments import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(Environment): #TODO implement GPU for matrix computations\n",
    "    \n",
    "    def __init__(self, specification): #TODO check for proper specification\n",
    "#         self.specification = dict()\n",
    "#         for key, value in specification.items():\n",
    "#             if key in ['environment', 'input_neurons', 'inter_neurons', 'output_neurons', 'connection_density', 'seed']:\n",
    "#                 if (key == 'environment' and not isinstance(value, Environment)) or \\\n",
    "#                 (key in ['input_neurons', 'inter_neurons', 'output_neurons', 'seed'] and not isinstance(value, int)) or \\\n",
    "#                 (key == 'connection_density' and not isinstance(value, float)):\n",
    "#                     raise TensorForceError('Invalid specification.')\n",
    "#                 self.specification[key] = value\n",
    "#             else:\n",
    "#                 raise TensorForceError('Invalid specification.')\n",
    "#         if set(specification.keys()) != set('environment', 'input_neurons', 'inter_neurons', 'output_neurons', 'neuroplasticity', 'max_history', 'lr'):\n",
    "#             raise TensorForceError('Invalid SNN specification')\n",
    "        self.specification = specification\n",
    "        self.potential_matrix = numpy.zeros((self.specification['input_neurons'] + self.specification['inter_neurons'] + self.specification['output_neurons'],))\n",
    "        self.weight_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "        self.weight_mask = numpy.ones_like(self.weight_matrix, dtype=numpy.uint8)\n",
    "        self.weight_mask[-self.specification['output_neurons']:, :self.specification['input_neurons']] = 0\n",
    "        numpy.fill_diagonal(self.weight_mask[:self.specification['inter_neurons'], -self.specification['inter_neurons']:], 0)\n",
    "        self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "        self.random_seed = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'SNN'\n",
    "    \n",
    "    def interconnect(self, weight_density):\n",
    "        sparse_matrix = scipy.sparse.random(self.weight_matrix.shape[0], self.weight_matrix.shape[1], density=weight_density, random_state=self.random_seed)\n",
    "        sparse_matrix.data *= 2\n",
    "        sparse_matrix.data -= 1\n",
    "        self.weight_matrix = numpy.multiply(sparse_matrix.toarray(), self.weight_mask)\n",
    "    \n",
    "    def close(self):\n",
    "        self.specification['environment'].close()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.next_input = self.specification['environment'].reset()\n",
    "        self.previous_reward = None\n",
    "        self.potential_matrix[:] = 0\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.history_matrix[:, :, :] = 0\n",
    "        return self.history_matrix\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self.random_seed = seed\n",
    "        return self.specification['environment'].seed(self.random_seed)\n",
    "    \n",
    "    def execute(self, actions):\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.weight_matrix = numpy.clip(numpy.multiply(numpy.add(self.weight_matrix, self.specification['lr'] * actions), self.weight_mask), -1, 1)\n",
    "        self.potential_matrix[:self.specification['input_neurons']] = numpy.add(self.potential_matrix[:self.specification['input_neurons']], self.next_input)\n",
    "        firing_matrix = numpy.vectorize(lambda x: x >= 1)(self.potential_matrix)\n",
    "        for i in range(self.specification['inter_neurons'] + self.specification['output_neurons']):\n",
    "            pos = self.specification['input_neurons'] + i\n",
    "            deltas = numpy.multiply(firing_matrix[:-self.specification['output_neurons']], self.weight_matrix[i])\n",
    "            if self.specification['neuroplasticity']:\n",
    "                self.history_matrix[i, self.specification['max_history'] - 1, :] = numpy.concatenate([self.potential_matrix[pos:pos + 1], firing_matrix[pos:pos + 1], deltas])\n",
    "            self.potential_matrix[pos] += numpy.sum(deltas)\n",
    "        self.potential_matrix = numpy.clip(numpy.multiply(self.potential_matrix, numpy.invert(firing_matrix)), -1, 1)\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.history_matrix = numpy.roll(self.history_matrix, 1, axis=1)\n",
    "        self.next_input, terminal, current_reward = self.specification['environment'].execute(firing_matrix[-self.specification['output_neurons']:])\n",
    "        reward = 0 if self.previous_reward is None else current_reward - self.previous_reward\n",
    "        self.previous_reward = current_reward\n",
    "        return self.history_matrix, terminal, reward\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        return {'shape': self.history_matrix.shape, 'type': 'float'}\n",
    "    \n",
    "    @property\n",
    "    def actions(self):\n",
    "        return {'shape': self.weight_matrix.shape, 'type': 'float', 'min_value': -1, 'max_value': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(Environment):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Test'\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = numpy.zeros((2,))\n",
    "        self.idx = 0\n",
    "        return self.state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        #print(actions)\n",
    "        #actions = numpy.round(actions)\n",
    "        self.state = numpy.add(self.state, numpy.array([actions['0'], actions['1']]))\n",
    "        terminal = self.idx >= 10\n",
    "        #terminal = True\n",
    "        #ones = numpy.count_nonzero(self.state)\n",
    "        reward = self.state[1] - self.state[0]\n",
    "#         self.state = numpy.abs(numpy.add(self.state, actions))\n",
    "#         terminal = min(self.state) <= 0\n",
    "#         reward = self.state[0] ** self.state[1]\n",
    "        self.idx += 1\n",
    "        return self.state, terminal, reward\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return {'shape': (2,), 'type': 'float'}\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        #return {'shape': (2,), 'type': 'float', 'min_value': 0, 'max_value': 1}\n",
    "        return {str(i): dict(type='float', continuous=True, min_value=0, max_value=1) for i in range(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "# Create an OpenAIgym environment\n",
    "env = Test()\n",
    "\n",
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='dense', size=32, activation='tanh'),\n",
    "    dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=8092,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=1000, max_episode_timesteps=1000, episode_finished=episode_finished)\n",
    "#runner.agent.save_model('/home/santiago/testmodel')\n",
    "#runner.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=numpy.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner = Runner(agent, env)\n",
    "# runner.run(episodes=1000, max_episode_timesteps=1000, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(agent, env)\n",
    "runner.agent.\n",
    "runner.agent.save_model('/home/santiago/testmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.agent.act(numpy.array([4, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner = Runner(agent=agent, environment=env)\n",
    "# runner.reset()\n",
    "# # Start learning\n",
    "# runner.run(episodes=3000, max_episode_timesteps=1000, episode_finished=episode_finished)\n",
    "# #runner.close()\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "#     ep=runner.episode,\n",
    "#     ar=numpy.mean(runner.episode_rewards[-100:]))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = SNN({'environment': Test(), 'input_neurons': 8, 'inter_neurons': 240, 'output_neurons': 8, 'max_history': 16, 'neuroplasticity': True, 'lr': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.interconnect(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# numpy.set_printoptions(threshold=numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = snn.weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(snn.weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.fromarray(snn.weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snn.weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = skimage.io.imread('/home/santiago/Pictures/412057.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skimage.io.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.DataFrame(data=snn.weight_matrix).to_csv('/home/santiago/Pictures/test.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = snn.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snn.next_input = numpy.random.randint(2, size=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    stuff = snn.execute(numpy.random.uniform(low=-1, high=1, size=snn.weight_matrix.shape))\n",
    "    #print(stuff[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(snn.potential_matrix.reshape((16, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.potential_matrix[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = numpy.array([0, 0, 1, 1, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
