{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import numpy\n",
    "import rl\n",
    "import scipy.sparse\n",
    "import skimage.io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(gym.Env):\n",
    "    \n",
    "    def __init__(self, specification):\n",
    "        self.specification = specification\n",
    "#         self.action_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "#         self.observation_space = gym.spaces.Box(-1, 1, (self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (self.specification['max_history'], 4))\n",
    "        self.potential_matrix = numpy.zeros((self.specification['input_neurons'] + self.specification['inter_neurons'] + self.specification['output_neurons'],))\n",
    "        self.weight_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons']))\n",
    "        self.weight_mask = numpy.ones_like(self.weight_matrix, dtype=numpy.uint8)\n",
    "        self.weight_mask[-self.specification['output_neurons']:, :self.specification['input_neurons']] = 0\n",
    "        numpy.fill_diagonal(self.weight_mask[:self.specification['inter_neurons'], -self.specification['inter_neurons']:], 0)\n",
    "#         self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['max_history'], self.specification['input_neurons'] + self.specification['inter_neurons'] + 2))\n",
    "        self.history_matrix = numpy.zeros((self.specification['inter_neurons'] + self.specification['output_neurons'], self.specification['input_neurons'] + self.specification['inter_neurons'], self.specification['max_history'], 4))\n",
    "        self.neuron_idx = 0\n",
    "        self.weight_idx = 0\n",
    "        self.random_seed = None\n",
    "        self.next_input = None\n",
    "        self.previous_reward = None\n",
    "    \n",
    "    def interconnect(self, weight_density):\n",
    "        sparse_matrix = scipy.sparse.random(self.weight_matrix.shape[0], self.weight_matrix.shape[1], density=weight_density, random_state=self.random_seed)\n",
    "        sparse_matrix.data *= 2\n",
    "        sparse_matrix.data -= 1\n",
    "        self.weight_matrix = numpy.multiply(sparse_matrix.toarray(), self.weight_mask)\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        self.specification['environment'].close()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.next_input = self.specification['environment'].reset()\n",
    "#         self.neuron_idx = 0\n",
    "#         self.weight_idx = 0\n",
    "        self.previous_reward = None\n",
    "        self.potential_matrix[:] = 0\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.history_matrix[:, :, :] = 0\n",
    "        return self.history_matrix[self.neuron_idx, self.weight_idx, :, :]\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.specification['environment'].render(mode)\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self.random_seed = seed\n",
    "        return self.specification['environment'].seed(self.random_seed)\n",
    "    \n",
    "    def step(self, action):\n",
    "        actions = numpy.full_like(self.weight_matrix, action - 1)\n",
    "#         self.neuron_idx += 1\n",
    "#         self.weight_idx += 1\n",
    "#         if self.neuron_idx == self.weight_matrix.shape[0]:\n",
    "#             self.neuron_idx = 0\n",
    "#         if self.weight_idx == self.weight_matrix.shape[1]:\n",
    "#             self.weight_idx = 0\n",
    "#         print(action)\n",
    "#         print(self.neuron_idx, self.weight_idx)\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.weight_matrix = numpy.clip(numpy.multiply(numpy.add(self.weight_matrix, self.specification['learning_rate'] * actions), self.weight_mask), -1, 1)\n",
    "#             self.weight_matrix[self.neuron_idx, self.weight_idx] += self.specification['learning_rate'] * (action - 1)\n",
    "#             self.weight_matrix = numpy.clip(numpy.multiply(self.weight_matrix, self.weight_mask), -1, 1)\n",
    "#         state = numpy.zeros_like(self.history_matrix[self.neuron_idx, self.weight_idx, :, :])\n",
    "#         reward = 0\n",
    "#         terminal = False\n",
    "#         if self.neuron_idx == 0 and self.weight_idx == 0:\n",
    "        self.potential_matrix[:self.specification['input_neurons']] = numpy.add(self.potential_matrix[:self.specification['input_neurons']], self.next_input)\n",
    "        firing_matrix = numpy.vectorize(lambda x: x >= 1)(self.potential_matrix)\n",
    "        for i in range(self.specification['inter_neurons'] + self.specification['output_neurons']):\n",
    "            pos = self.specification['input_neurons'] + i\n",
    "            deltas = numpy.multiply(firing_matrix[:-self.specification['output_neurons']], self.weight_matrix[i])\n",
    "            delta = numpy.sum(deltas)\n",
    "            if self.specification['neuroplasticity']:\n",
    "                self.history_matrix[i, self.weight_idx, self.specification['max_history'] - 1, :] = numpy.array([deltas[self.weight_idx], delta, self.potential_matrix[pos], firing_matrix[pos]])\n",
    "            self.potential_matrix[pos] += delta\n",
    "        self.potential_matrix = numpy.clip(numpy.multiply(self.potential_matrix, numpy.invert(firing_matrix)), -1, 1)\n",
    "        if self.specification['neuroplasticity']:\n",
    "            self.history_matrix = numpy.roll(self.history_matrix, 2, axis=1)\n",
    "        state = self.history_matrix[self.neuron_idx, self.weight_idx, :, :]\n",
    "        self.next_input, reward, terminal, info = self.specification['environment'].step(firing_matrix[-self.specification['output_neurons']:].astype(int))\n",
    "#             reward = 0 if self.previous_reward is None else current_reward - self.previous_reward\n",
    "#             self.previous_reward = current_reward\n",
    "#         if self.weight_idx == self.specification['input_neurons'] + self.specification['inter_neurons'] - 1:\n",
    "#             self.weight_idx = 0\n",
    "#             if self.neuron_idx == self.specification['inter_neurons'] + self.specification['output_neurons'] - 1:\n",
    "#                 self.neuron_idx = 0\n",
    "#             else:\n",
    "# #                 print(self.neuron_idx)\n",
    "#                 self.neuron_idx += 1\n",
    "#         else:\n",
    "#             self.weight_idx += 1\n",
    "        return state, reward, terminal, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 8\n",
    "inter_neurons = 128\n",
    "output_neurons = 8\n",
    "max_history = 16\n",
    "hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test2(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Box(0, 1, (1,))\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (1,))\n",
    "        self.state = None\n",
    "        self.idx = None\n",
    "        self.random_seed = None\n",
    "        self.previous_action = None\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self.random_seed = seed\n",
    "        random.seed(seed)\n",
    "        return seed\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = numpy.ones((1,))\n",
    "        self.idx = 0\n",
    "        self.previous_action = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "#         print(action)\n",
    "#         print(self.idx)\n",
    "#         self.state[action] = int(not bool(self.state[action]))\n",
    "#         self.state = numpy.zeros((16,))\n",
    "#         self.state[self.idx] = 1\n",
    "        self.idx += 1\n",
    "        terminal = self.idx == 100\n",
    "        reward = 0.0\n",
    "#         if action[0] == 0 and self.previous_action == 0:\n",
    "#             reward = 1.0\n",
    "#         else:\n",
    "#             reward = -1000\n",
    "        if action[0] == 1:\n",
    "            reward = 1.0\n",
    "        elif self.previous_action == 0:\n",
    "            reward = 0.0\n",
    "        self.previous_action = action[0]\n",
    "        return self.state, reward, terminal, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'SNN'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "#env = gym.make(ENV_NAME)\n",
    "env = SNN({'environment': Test2(), 'input_neurons': 1, 'inter_neurons': 64, 'output_neurons': 1, 'max_history': 16, 'neuroplasticity': True, 'learning_rate': 0.01})\n",
    "numpy.random.seed(0)\n",
    "env.seed(0)\n",
    "env.interconnect(0.1)\n",
    "nb_actions = env.action_space.n\n",
    "#functools.reduce(operator.mul, env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/.local/lib/python3.6/site-packages/skimage/io/_plugins/matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f959de555f8>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEYCAYAAADPkTRJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f959f48b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skimage.io.imshow(env.weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_36 (Reshape)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 6,819\n",
      "Trainable params: 6,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "   100/50000: episode: 1, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 26.000, mean reward: 0.260 [0.000, 1.000], mean action: 1.030 [0.000, 2.000], mean observation: 0.012 [0.000, 1.441], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   200/50000: episode: 2, duration: 1.797s, episode steps: 100, steps per second: 56, episode reward: 7.000, mean reward: 0.070 [0.000, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.013 [0.000, 2.022], loss: 0.086943, mean_absolute_error: 0.241177, mean_q: 0.273670\n",
      "   300/50000: episode: 3, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 5.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.040 [0.000, 2.000], mean observation: 0.011 [0.000, 1.361], loss: 0.055516, mean_absolute_error: 0.315352, mean_q: 0.405008\n",
      "   400/50000: episode: 4, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 10.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: 0.011 [0.000, 1.361], loss: 0.045912, mean_absolute_error: 0.361255, mean_q: 0.492520\n",
      "   500/50000: episode: 5, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 34.000, mean reward: 0.340 [0.000, 1.000], mean action: 0.910 [0.000, 2.000], mean observation: 0.020 [0.000, 3.825], loss: 0.064372, mean_absolute_error: 0.480759, mean_q: 0.637176\n",
      "   600/50000: episode: 6, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 2.000, mean reward: 0.020 [0.000, 1.000], mean action: 0.890 [0.000, 2.000], mean observation: 0.011 [0.000, 1.341], loss: 0.069986, mean_absolute_error: 0.579494, mean_q: 0.776409\n",
      "   700/50000: episode: 7, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.980 [0.000, 2.000], mean observation: 0.009 [0.000, 1.121], loss: 0.056669, mean_absolute_error: 0.638184, mean_q: 0.878990\n",
      "   800/50000: episode: 8, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.040 [0.000, 2.000], mean observation: 0.010 [0.000, 1.101], loss: 0.053060, mean_absolute_error: 0.698765, mean_q: 0.992107\n",
      "   900/50000: episode: 9, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.010 [0.000, 2.000], mean observation: 0.010 [0.000, 1.181], loss: 0.050990, mean_absolute_error: 0.739996, mean_q: 1.058684\n",
      "  1000/50000: episode: 10, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.080 [0.000, 2.000], mean observation: 0.010 [0.000, 1.201], loss: 0.042995, mean_absolute_error: 0.769407, mean_q: 1.109175\n",
      "  1100/50000: episode: 11, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 14.000, mean reward: 0.140 [0.000, 1.000], mean action: 1.090 [0.000, 2.000], mean observation: 0.012 [0.000, 1.391], loss: 0.046595, mean_absolute_error: 0.836516, mean_q: 1.216185\n",
      "  1200/50000: episode: 12, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 46.000, mean reward: 0.460 [0.000, 1.000], mean action: 1.070 [0.000, 2.000], mean observation: 0.020 [-0.078, 3.266], loss: 0.056659, mean_absolute_error: 0.917433, mean_q: 1.306758\n",
      "  1300/50000: episode: 13, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 48.000, mean reward: 0.480 [0.000, 1.000], mean action: 1.030 [0.000, 2.000], mean observation: 0.026 [0.000, 4.874], loss: 0.074704, mean_absolute_error: 1.028960, mean_q: 1.460618\n",
      "  1400/50000: episode: 14, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 48.000, mean reward: 0.480 [0.000, 1.000], mean action: 0.970 [0.000, 2.000], mean observation: 0.040 [0.000, 10.220], loss: 0.104417, mean_absolute_error: 1.159111, mean_q: 1.638169\n",
      "  1500/50000: episode: 15, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 48.000, mean reward: 0.480 [0.000, 1.000], mean action: 0.940 [0.000, 2.000], mean observation: 0.027 [0.000, 4.211], loss: 0.149062, mean_absolute_error: 1.330738, mean_q: 1.890764\n",
      "  1600/50000: episode: 16, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 48.000, mean reward: 0.480 [0.000, 1.000], mean action: 1.010 [0.000, 2.000], mean observation: 0.029 [0.000, 5.149], loss: 0.122269, mean_absolute_error: 1.488357, mean_q: 2.127188\n",
      "  1700/50000: episode: 17, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 28.000, mean reward: 0.280 [0.000, 1.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.017 [-0.288, 3.573], loss: 0.135354, mean_absolute_error: 1.622578, mean_q: 2.326223\n",
      "  1800/50000: episode: 18, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 3.000, mean reward: 0.030 [0.000, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.012 [0.000, 1.326], loss: 0.137079, mean_absolute_error: 1.781111, mean_q: 2.546039\n",
      "  1900/50000: episode: 19, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.930 [0.000, 2.000], mean observation: 0.010 [0.000, 1.146], loss: 0.154111, mean_absolute_error: 1.877457, mean_q: 2.702195\n",
      "  2000/50000: episode: 20, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.008 [0.000, 1.000], loss: 0.143036, mean_absolute_error: 1.982916, mean_q: 2.863593\n",
      "  2100/50000: episode: 21, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.050 [0.000, 2.000], mean observation: 0.007 [0.000, 1.000], loss: 0.124795, mean_absolute_error: 2.061271, mean_q: 2.983963\n",
      "  2200/50000: episode: 22, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.060 [0.000, 2.000], mean observation: 0.009 [0.000, 1.000], loss: 0.144209, mean_absolute_error: 2.138747, mean_q: 3.099797\n",
      "  2300/50000: episode: 23, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.009 [0.000, 1.026], loss: 0.133340, mean_absolute_error: 2.215852, mean_q: 3.207341\n",
      "  2400/50000: episode: 24, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.008 [0.000, 1.000], loss: 0.158954, mean_absolute_error: 2.310075, mean_q: 3.345471\n",
      "  2500/50000: episode: 25, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.006 [0.000, 1.000], loss: 0.163764, mean_absolute_error: 2.375724, mean_q: 3.464384\n",
      "  2600/50000: episode: 26, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.002 [-0.448, 0.206], loss: 0.170741, mean_absolute_error: 2.460244, mean_q: 3.597127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2700/50000: episode: 27, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.970 [0.000, 2.000], mean observation: -0.004 [-1.000, 0.146], loss: 0.158694, mean_absolute_error: 2.522339, mean_q: 3.671645\n",
      "  2800/50000: episode: 28, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.760 [0.000, 2.000], mean observation: -0.003 [-1.000, 0.166], loss: 0.192029, mean_absolute_error: 2.572355, mean_q: 3.742008\n",
      "  2900/50000: episode: 29, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.900 [0.000, 2.000], mean observation: -0.009 [-1.000, 0.000], loss: 0.228845, mean_absolute_error: 2.590037, mean_q: 3.797327\n",
      "  3000/50000: episode: 30, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.990 [0.000, 2.000], mean observation: -0.010 [-1.000, 0.000], loss: 0.156503, mean_absolute_error: 2.633696, mean_q: 3.861550\n",
      "  3100/50000: episode: 31, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.009 [-1.000, 0.000], loss: 0.166292, mean_absolute_error: 2.680101, mean_q: 3.929529\n",
      "  3200/50000: episode: 32, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.008 [-1.000, 0.000], loss: 0.152150, mean_absolute_error: 2.712609, mean_q: 3.973861\n",
      "done, took 24.900 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f959e6f9eb8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "# model = Sequential()\n",
    "# model.add(Reshape((16, 4), input_shape=(1, 16, 4)))\n",
    "# model.add(Bidirectional(LSTM(32, return_sequences=True), merge_mode='concat'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Bidirectional(LSTM(32), merge_mode='concat'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(3, activation='linear'))\n",
    "# print(model.summary())\n",
    "model = Sequential()\n",
    "model.add(Reshape((64,), input_shape=(1, 16, 4)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = EpsGreedyQPolicy(0.99)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
